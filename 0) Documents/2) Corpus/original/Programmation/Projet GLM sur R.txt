---
title: "Projet – Modélisation GLM avec R"
author: "Adrien Moyaux & Jonathan Munier"
date: "Dimanche 5 juin 2022"
output: 
  pdf_document : 
    toc : true
    toc_depth : 3
---

{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)




\newpage



# 1/ Traitements préliminaires sur la base de données

Dans ce projet, nous allons ajuster un modèle GLM sur le jeu de données *MTPL*. Celui-ci est composé de contrats d'assurance en Suisse pour la garantie automobile. L'objectif est de prédire le nombre de sinistres (*claims*) en fonction des différentes variables connues. Notre modèle final ne contiendra que des variables catégorielles.


\vspace*{2cm}


## Importation des données

Chargeons tout d'abord le fichier d'outils.
{r Importation fichier outil, cache = TRUE}
source("http://www-irma.u-strasbg.fr/~jberard/outils-R-GLM.R")


Importons à présent la base de données :
{r Importation données, cache = TRUE}
donnees_brutes <- read.csv2("MTPL_data.csv", stringsAsFactors = TRUE)
str(donnees_brutes)
summary(donnees_brutes)


Utilisons la commande *set.seed* pour garantir la reproductibilité des tirages pseudo-aléatoires :
{r, cache = TRUE}
set.seed(2022)


Enfin, importons la librairie *tidyverse* :
{r, message = FALSE, warning = FALSE}
library(tidyverse)





\vspace*{3cm}



## Gestion des valeurs manquantes

{r, cache = TRUE}
donnees_brutes[!complete.cases(donnees_brutes), ] # Affichage des lignes contenant des NA


Toutes les lignes de notre base de données sont donc bien complètes. Cette vérification étant faite, nous pouvons commencer le retraitement des variables.



\vspace*{3cm}



## Conversion correcte (numérique) des variables 

Les variables *expo* et *truefreq* sont des variables quantitatives. Or, elles sont sous forme de variables catégorielles dans la base de données. Nous allons donc les convertir pour les mettre au bon format.

{r Conversion des types, cache = TRUE}
donnees_brutes$expo <- as.numeric(as.character(donnees_brutes$expo))
donnees_brutes$truefreq <-
  as.numeric(as.character(donnees_brutes$truefreq))
donnees <- donnees_brutes
donnees_cat <- donnees




\newpage



# 2/ Regroupement a priori des catégories


\vspace*{2cm}


## 2.1/ Regroupement des variables catégorielles

Dans cette partie, nous allons examiner les différentes modalités des variables catégorielles. Si l'une d'entre elles s'avère ^etre trop "petite", c'est-à-dire qu'elle ne contient pas beaucoup de valeurs, nous regrouperons cette modalité avec une autre.

Ces manipulations permettront d'avoir une exposition suffisante sur chacune des modalités, et d'ainsi aboutir par la suite à un modèle plus juste. Dans une partie ultérieure, nous déterminorons s'il est judicieux de fusionner certaines des modalités restantes selon leur contribution au modèle linéaire généralisé.

Les variables catégorielles sont *gas*, *brand*, *area* et *ct*.


\vspace*{2cm}


### Variable *gas*

{r, cache = TRUE}
table_gas <- table(donnees$gas)
barplot(table_gas)

Aucun retraitement n'est à effectuer sur cette variable, celle-ci n'ayant que deux modalités de taille équivalente.


\vspace*{2cm}


### Variable *brand*

{r, cache = TRUE}
table_brand <- table(donnees$brand)
barplot(table_brand, main = "Nombre de polices par marque")
table_brand


La marque *B14* est celle contenant le moins de polices. Cependant, il ne semble pas judicieux de l'intégrer dans une autre catégorie. En effet, elle contient tout de m^eme 2933 polices, ce qui est un nombre non négligeable.

{r, cache = TRUE}
table_expo_brand <- xtabs(expo ~ brand, data = donnees)
barplot(table_expo_brand, main = "Exposition par marque")
table_expo_brand


En regardant l'exposition, c'est encore la marque *B14* qui a l'exposition la plus faible. Toutefois, l'exposition totale de cette marque est de 1649 années, ce qui est largement suffisant pour laisser cette catégorie telle quelle.


\vspace*{2cm}


### Variable *area*

{r, cache = TRUE}
table_area <- table(donnees$area)
barplot(table_area, main = "Nombre de polices par type de zone")
table_area

table_expo_area <- xtabs(expo ~ area, data = donnees)
barplot(table_expo_area, main = "Exposition par type de zone")
table_expo_area


La zone F est celle ayant le moins de polices. Ici aussi, il ne semble pas utile de modifier cette catégorie car elle est suffisamment grande.


\vspace*{2cm}


### Variable *ct*

{r, cache = TRUE}
table_ct <- table(donnees$ct)
barplot(table_ct, main = "Nombre de polices par canton")
table_ct

table_expo_ct <- xtabs(expo ~ ct, data = donnees)
barplot(table_expo_ct, main = "Exposition par canton")
table_expo_ct


{r, cache = TRUE, message = FALSE, warning = FALSE}
library(rgdal)
library(sp)
cantons <- readOGR(dsn = "swisscantonsmod-master", layer = "ch-cantons")
noms_cantons <- 
  c("AG","AR", "AI","BL", "BS", "BE", "FR", "GE", "GL", "GR",
    "JU", "LU", "NE", "NW", "OW", "SG", "SH", "SZ", "SO", "TG",
    "TI", "UR", "VS", "VD", "ZH", "ZG")
cantons$expo <- as.numeric(table_expo_ct[noms_cantons])
spplot(cantons,
       "expo",
       col.regions = rev(gray.colors(100, start = 0, end = 1)),
       main = "Exposition par canton")


Tous les cantons sont suffisamment bien représentés, donc aucune de ces modalités n'est trop "petite".



\newpage



## 2.2/ Regroupement des variables quantitatives

Les variables catégorielles n'ont donc a priori pas besoin d'^etre recatégorisées. Cependant, ce sera le cas pour les variables quantitatives. En effet, nous allons les regrouper dans des classes de valeurs.

Les variables explicatives quantitatives sont : *age*, *ac*, *power* et *dens*.


\vspace*{2cm}


### Variable *age*

{r, cache = TRUE}
table_age <- table(donnees$age)
barplot(table_age, main = "Nombre de polices par âge")
head(table_age, n = 10L) ; tail(table_age, n = 15L)

table_expo_age <- xtabs(expo ~ age, data = donnees)
barplot(table_expo_age, main = "Exposition par âge")
head(table_expo_age, n = 10L) ; tail(table_expo_age, n = 15L)


Il y a peu de contrats pour les individus en-dessous de 20 ans. De m^eme, après 78 ans, le nombre de contrats pour chaque classe d'âge est faible.

Par conséquent, nous allons regrouper les valeurs extr^emes pour avoir une exposition supérieure à 1000 années police dans chaque catégorie d'âge. Hormis cela, nous laisserons (pour le moment) une catégorie par an entre 20 et 78 ans.

{r, cache = TRUE}
donnees_cat$age <-
  cut(donnees$age,
      breaks = c(18, (20:78), 80, 82, 90),
      include.lowest = TRUE)

table_expo_age = xtabs(expo ~ age, data = donnees_cat)
barplot(table_expo_age, main = "Exposition par âge")

Comme le montre notre graphique, la nouvelle catégorisation de l'âge permet une exposition suffisante pour chaque classe.


\vspace*{2cm}


### Variable *ac*

{r, cache = TRUE}
table_ac <- table(donnees$ac)
barplot(table_ac, main = "Nombre de polices par âge du véhicule")
table_ac

table_expo_ac <- xtabs(expo ~ ac, data = donnees)
barplot(table_expo_ac, main = "Exposition par par âge du véhicule")
table_expo_ac


À partir de 20 ans, le nombre de contrats est extr^emement faible. Ainsi, nous allons laisser (pour le moment) une catégorie par an entre 0 et 20 ans. Les âges au-delà de 20 ans seront regroupés dans la classe (20, 35] qui aura alors une exposition suffisante.

{r, cache = TRUE}
donnees_cat$ac <-
  cut(donnees$ac,
      breaks = c((0:20), 35),
      include.lowest = TRUE)

table_expo_ac <- xtabs(expo ~ ac, data = donnees_cat)
barplot(table_expo_ac, main = "Exposition par âge du véhicule")


Comme le montre notre graphique, la nouvelle catégorisation de l'âge du véhicule permet une exposition suffisante pour chaque classe.


\vspace*{2cm}


### Variable *power*

{r, cache = TRUE}
table_power <- table(donnees$power)
barplot(table_power, main = "Nombre de polices par puissance")
table_power

table_expo_power <- xtabs(expo ~ power, data = donnees)
barplot(table_expo_power, main = "Exposition par puissance")
table_expo_power

Les véhicules ayant une puissance de 10 et plus sont très minoritaires. Nous allons donc les regrouper dans une m^eme classe. Hormis cela, nous laisserons (pour le moment) une catégorie pour chaque valeur entre 0 et 10.

{r, cache = TRUE}
donnees_cat$power <-
  cut(donnees$power,
      breaks = c((0:10), 12),
      include.lowest = TRUE)

table_expo_power <- xtabs(expo ~ power, data = donnees_cat)
barplot(table_expo_power, main = "Exposition par puissance")



\vspace*{2cm}


### Variable *dens*

{r, cache = TRUE}
hist(donnees$dens, main = "Histogramme de la densité")


La variable *dens* étant une variable continue, nous allons réaliser une catégorisation en déciles :
{r, cache = TRUE}
donnees_cat$dens <- cut(donnees$dens,
                        breaks = quantile(donnees$dens, seq(0, 1, by = 0.1)),
                        include.lowest = TRUE)
table_expo_dens <- xtabs(expo ~ dens, data = donnees_cat)
barplot(table_expo_dens, main = "Exposition par densité de population")




\newpage


## 2.3/ Conclusion

{r, cache = TRUE}
recat = function(donnees){
  donnees_cat <- donnees


  
  # Modifications de la partie 2 (catégorisation a priori)
  
  donnees_cat$age <-
    cut(donnees$age,
        breaks = c(18, (20:78), 80, 82, 90),
        include.lowest = TRUE)
  
  donnees_cat$ac <-
    cut(donnees$ac,
        breaks = c((0:20), 35),
        include.lowest = TRUE)
  
  donnees_cat$power <-
    cut(donnees$power,
        breaks = c((0:10), 12),
        include.lowest = TRUE)
  
  donnees_cat$dens <- cut(donnees$dens,
                          breaks = quantile(donnees$dens, seq(0, 1, by = 0.1)),
                          include.lowest = TRUE)
  
  
  
  donnees_cat
}



donnees_cat <- recat(donnees)




\newpage



# 3/ Élimination de variables explicatives par VIF

À l'issue de la partie précédente, nous obtenons la base de données *donnees_cat*. Celle-ci correspond à la base *donnees* sur laquelle les regroupements a priori des catégories ont été réalisés. Nous allons maintenant aller plus loin en regardant à l'élimination éventuelle de certaines variables.


\vspace*{2cm}


## Examen préliminaire des dépendances entre variables

{r Corrplot, message = FALSE, warning = FALSE, cache = TRUE}
library(questionr)
V_de_Cramer <-
  Vectorize(
    FUN = function(u, v, donnees) {
      cramer.v(table(donnees[, u], donnees[, v]))
    },
    vectorize.args = c("u", "v")
  )
noms_explicat_cat <-
  c("age", "ac", "power", "gas", "brand", "area", "dens", "ct")
C_cat <-
  outer(
    X = noms_explicat_cat,
    Y = noms_explicat_cat,
    FUN = "V_de_Cramer",
    donnees = donnees_cat
  )
colnames(C_cat) <- noms_explicat_cat
rownames(C_cat) <- noms_explicat_cat
C_cat

library(corrplot)
corrplot(C_cat)


La seule dépendance notable au sein d'une paire de variables est celle qui appara^it entre *area* et *dens*.

Nous allons donc dans la suite essayer de trouver un modèle dans lequel une de ces deux variables sera éliminée, ce qui nous permettra d'interpréter plus facilement les coefficients.


\vspace*{2cm}


## Ajustement GLM et VIF

Effectuons un ajustement GLM log-Poisson sur *donnees_cat*, qui contient les variables explicatives (légèrement) catégorisées.

{r Ajustement 1, cache = TRUE}
ajustement_1 = glm(
  claims ~
    age +
    ac +
    power +
    gas +
    brand +
    area +
    dens +
    ct +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_cat
)
summary(ajustement_1)


L'ajustement étant effectué, passons au calcul des VIF généralisés (un par variable catégorielle).

{r, message = FALSE, warning = FALSE}
library(car)
vif(ajustement_1)


Comme l'on pouvait s'y attendre compte tenu de la dépendance observée dans la partie précédente, les variables *area* et *dens* présentent une valeur sensiblement supérieure à 1.

Pour choisir la variable qui sera éliminée, nous allons comparer en terme d'AIC l'impact de la modification de la suppression de chacune de ces variables :

{r Ajustement 1 sans dens ou area, cache = TRUE}
ajustement_1_sans_dens <- glm(
  claims ~
    age +
    ac +
    power +
    gas +
    brand +
    area +
    ct +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_cat
)

summary(ajustement_1_sans_dens)



ajustement_1_sans_area <- glm(
  claims ~
    age +
    ac +
    power +
    gas +
    brand +
    dens +
    ct +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_cat
)

summary(ajustement_1_sans_area)


{r Comparaison AIC ajustement 1}
ajustement_1.liste_aic <-
  list(
    list(nom = "ajustement complet",
         aic = AIC(ajustement_1)),
    list(nom = "ajustement sans dens",
         aic = AIC(ajustement_1_sans_dens)),
    list(nom = "ajustement sans area", aic = AIC(ajustement_1_sans_area))
  )

library(rlist)
list.sort(ajustement_1.liste_aic, aic)


Le plus petit AIC est obtenu en éliminant la variable *dens*. **C'est donc le modèle sans la variable *dens* qui est retenu pour la suite.**

Vérifions qu'il n'est pas nécessaire de retirer d'autres variables :

{r, cache = TRUE}
vif(ajustement_1_sans_dens)


Les VIF obtenus ont des valeurs voisines de 1, par conséquent nous conserverons toutes les variables restantes.


\vspace*{2cm}


## Conclusion

Le premier ajustement effectué est *ajustement_1* dont l'AIC est de `r ajustement_1$aic`. En supprimant la variable *dens*, l'AIC de *ajustement_1_sans_dens* est de `r ajustement_1_sans_dens$aic`.



\newpage



# 4/ Regroupement a posteriori des catégories avec Lasso

D'après la partie 3, les variables explicatives pertinentes pour notre modèle sont *age*, *ac*, *power*, *gas*, *brand*, *area* et *ct* (donc toutes les variables sauf *dens*).

Le jeu de données sur lequel le modèle va tourner est *donnees_cat* (cf. la partie 2). Nous allons maintenant encore fusionner certaines catégories de ce jeu de données afin de simplifier le modèle. Pour cela, nous utiliserons la méthode LASSO.

{r, Ajustement lasso smurf, cache = TRUE}
library(smurf)

ajustement_lasso_smurf <- glmsmurf(
  claims ~
    p(age, pen = "flasso") +
    p(ac, pen = "flasso") +
    p(power, pen = "flasso") +
    p(gas, pen = "lasso") +
    p(brand, pen = "gflasso") +
    p(area, pen = "flasso") +
    p(ct, pen = "gflasso"),
  offset = log(donnees_cat$expo),
  family = poisson(link = "log"),
  data = donnees_cat,
  pen.weights = "glm.stand",
  lambda = "is.aic",
  control = list(lambda.reest = TRUE,
                 print = TRUE)
)

summary(ajustement_lasso_smurf)


{r, cache = TRUE}
recategorisation_lasso <- categ_smurf(ajustement_lasso_smurf)
recategorisation_lasso


Pour les trois variables *age*, *ac* et *power*, nous allons simplifier le nom de certains regroupements de catégories (par exemple "(12,18]" au lieu de "(12,13]+(13,14]+(14,15]+(15,16]+(16,17]+(17,18]").

{r, cache = TRUE}
noms_var_cut <- c("age", "ac", "power")
for (nom_variable in noms_var_cut) {
  recategorisation_lasso[[nom_variable]] <-
    arrange_recat_cut(recategorisation_lasso[[nom_variable]])$recat
}


Regardons à présent la carte des cantons après fusion.

{r, cache = TRUE}
f_recat <- list()
for (i in (1:length(recategorisation_lasso))) {
  f_recat[[names(recategorisation_lasso)[i]]] <-
    cree_fonction_recat(recategorisation_lasso[[i]])
}

cantons$fus <-
  f_recat[["ct"]](factor(noms_cantons, levels = levels(donnees$ct)))
spplot(cantons,
       "fus",
       col.regions = rainbow(length(recategorisation_lasso[["ct"]])),
       main = "Fusion des cantons")


À présent, créons le modèle GLM avec la fusion des catégories :

{r Ajustement fusion lasso, cache = TRUE}
explicatives <- lapply(
  X = names(recategorisation_lasso),
  FUN = function(x) {
    paste('f_recat[[', '"', x, '"]](', x, ')', sep = "")
  }
)
droite <- do.call(what = paste, args = c(explicatives, sep = "+"))
gauche <- "claims~"
formule_glm <-
  as.formula(paste(gauche, droite, "+offset(log(expo))", sep = ""))
formule_glm

ajustement_fus_lasso <-
  glm(formule_glm, family = poisson(link = "log"), data = donnees_cat)

summary(ajustement_fus_lasso)


Voici la variation d'AIC après fusion des catégories :

{r, cache = TRUE}
AIC(ajustement_1_sans_dens)
AIC(ajustement_fus_lasso)

(AIC(ajustement_fus_lasso) - AIC(ajustement_1_sans_dens)) / AIC(ajustement_1_sans_dens)


Observons à présent graphiquement la modification des fusions de catégories pour chaque variable.


\vspace*{2cm}


## Variable *age*

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "age",
  donnees = donnees_cat
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "age",
  nom_expo = "expo",
  nom_terme = 'f_recat[["age"]](age)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)




\vspace*{2cm}


## Variable *ac*

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "ac",
  donnees = donnees_cat
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "ac",
  nom_expo = "expo",
  nom_terme = 'f_recat[["ac"]](ac)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)



\vspace*{2cm}


## Variable *power*

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "power",
  donnees = donnees_cat,
  facteur_dessus = 2,
  facteur_dessous = 2
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "power",
  nom_expo = "expo",
  nom_terme = 'f_recat[["power"]](power)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)



\vspace*{2cm}


## Variable *gas*

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "gas",
  donnees = donnees_cat,
  facteur_dessus = 2,
  facteur_dessous = 2
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "gas",
  nom_expo = "expo",
  nom_terme = 'f_recat[["gas"]](gas)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)



\vspace*{2cm}


## Variable *brand**=="=*i"=" = " = "

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "brand",
  donnees = donnees_cat
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "brand",
  nom_expo = "expo",
  nom_terme = 'f_recat[["brand"]](brand)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)




\vspace*{2cm}


## Variable *area*

{r, message = FALSE, warning = FALSE, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "area",
  donnees = donnees_cat
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "area",
  nom_expo = "expo",
  nom_terme = 'f_recat[["area"]](area)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)


{r, cache = TRUE}
trace_moyenne_pond_expo_categ(
  nom_expo = "expo",
  nom_expo = "expo",
  nom_expo = "expo",
  nom_reponse = "claims",
  nom_exposition = "expo",
  nom_variable = "ct",
  donnees = donnees_cat
)

trace_terme_univar(
  ajustement = ajustement_fus_lasso,
  nom_variable = "ct",
  nom_terme = 'f_recat[["ct"]](ct)',
  trace_expo = TRUE,
  trace_int_conf = TRUE
)



\vspace*{2cm}


## Conclusion

Le jeu de données *donnees_cat_lasso* correspond à *donnees_cat* où la fusion opérée par Lasso y est effectuée.

{r, cache = TRUE}
library(tidyverse)



recat_lasso = function(donnees){
  donnees_cat <- donnees
  
  
  
  # Modifications de la partie 2 (catégorisation a priori)
  
  donnees_cat$age <-
    cut(donnees$age,
        breaks = c(18, (20:78), 80, 82, 90),
        include.lowest = TRUE)
  
  donnees_cat$ac <-
    cut(donnees$ac,
        breaks = c((0:20), 35),
        include.lowest = TRUE)
  
  donnees_cat$power <-
    cut(donnees$power,
        breaks = c((0:10), 12),
        include.lowest = TRUE)
  
  donnees_cat$dens <- cut(donnees$dens,
                          breaks = quantile(donnees$dens, seq(0, 1, by = 0.1)),
                          include.lowest = TRUE)
  
  
  
  # Modifications de la partie 4 (catégorisation trouvée par Lasso)
  donnees_cat_lasso <-
    donnees_cat %>% mutate(age = factor(
      case_when(
        age %in% c("(24,25]", "(25,26]") ~ "(24,26]",
        age %in% c("(26,27]", "(27,28]") ~ "(26,28]",
        age %in% c("(28,29]", "(29,30]") ~ "(28,30]",
        age %in% c("(30,31]", "(31,32]", "(32,33]") ~ "(30,33]",
        age %in% c("(33,34]", "(34,35]") ~ "(33,35]",
        age %in% c("(35,36]", "(36,37]", "(37,38]", "(38,39]", "(39,40]") ~ "(35,40]",
        age %in% c("(40,41]", "(41,42]", "(42,43]", "(43,44]", "(44,45]") ~ "(40,45]",
        age %in% c("(45,46]", "(46,47]") ~ "(45,47]",
        age %in% c("(47,48]", "(48,49]") ~ "(47,49]",
        age %in% c(
          "(49,50]",
          "(50,51]",
          "(51,52]",
          "(52,53]",
          "(53,54]",
          "(54,55]",
          "(55,56]"
        ) ~ "(49,56]",
        age %in% c("(56,57]", "(57,58]") ~ "(56,58]",
        age %in% c("(58,59]", "(59,60]", "(60,61]") ~ "(58,61]",
        age %in% c("(61,62]", "(62,63]", "(63,64]", "(64,65]") ~ "(61,65]",
        age %in% c("(66,67]", "(67,68]") ~ "(66,68]",
        age %in% c("(68,69]", "(69,70]", "(70,71]") ~ "(68,71]",
        age %in% c("(71,72]", "(72,73]") ~ "(71,73]",
        age %in% c("(74,75]", "(75,76]", "(76,77]", "(77,78]") ~ "(74,78]",
        TRUE ~ as.character(age)
      )
    )) %>% mutate(ac = factor(
        case_when(
          ac %in% c("(2,3]", "(3,4]", "(4,5]") ~ "(2,5]",
          ac %in% c("(6,7]", "(7,8]", "(8,9]", "(9,10]", "(10,11]", "(11,12]") ~ "(6,12]",
          ac %in% c(
            "(12,13]",
            "(13,14]",
            "(14,15]",
            "(15,16]",
            "(16,17]",
            "(17,18]"
          ) ~ "(12,18]",
          ac %in% c("(18,19]", "(19,20]") ~ "(18,20]",
          TRUE ~ as.character(ac)
        )
    )) %>% mutate(power = factor(
        case_when(
          power %in% c("(1,2]", "(2,3]") ~ "(1,3]",
          power %in% c("(8,9]", "(9,10]", "(10,12]") ~ "(8,12]",
          TRUE ~ as.character(power)
        )
    )) %>% mutate(brand = factor(
        case_when(
          brand %in% c("B1", "B10", "B3", "B6") ~ "B1+B10+B3+B6",
          brand %in% c("B11", "B13", "B5") ~ "B11+B13+B5",
          brand %in% c("B14", "B2", "B4") ~ "B14+B2+B4",
          TRUE ~ as.character(brand)
        )
    )) %>% mutate(ct = factor(
        case_when(
          ct %in% c("AG", "FR", "GL", "GR", "VS", "ZG") ~ "AG+FR+GL+GR+VS+ZG",
          ct %in% c("AI", "JU", "SH", "TG") ~ "AI+JU+SH+TG",
          ct %in% c("BL", "SO", "UR", "ZH") ~ "BL+SO+UR+ZH",
          ct %in% c("GE", "NE", "VD") ~ "GE+NE+VD",
          ct %in% c("LU", "NW", "SG") ~ "LU+NW+SG",
          ct %in% c("OW", "SZ") ~ "OW+SZ",
          TRUE ~ as.character(ct)
        )
    ))
  
  
  
  donnees_cat_lasso
}

donnees_cat_lasso <- recat_lasso(donnees)



head(donnees_cat)
head(donnees_cat_lasso)



{r, cache = TRUE}
ajustement_lasso = glm(
  claims ~
    age +
    ac +
    power +
    gas +
    brand +
    area +
    dens +
    ct +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_cat_lasso
)


Le premier ajustement effectué était *ajustement_1* dont l'AIC est de `r ajustement_1$aic`. Le second ajustement, *ajustement_1_sans_dens*, a été obtenu en supprimant la variable *dens*. Son AIC est de `r ajustement_1_sans_dens$aic`. Enfin, le troisième ajustement, conclusion de cette partie, est *ajustement_lasso*. Il est obtenu en fusionnant des catégories par Lasso. Son AIC est de `r ajustement_lasso$aic`.



\newpage



# 5/ Inclusion de termes d'interaction

Suite à la partie précédente, notre jeu de données est désormais *donnees_cat_lasso*. Ce dernier correspond à *donnees_cat* après la fusion par lasso. Pour rappel, *donnees_cat* fait référence aux données de départ auxquelles a été appliqué une catégorisation a priori. Cette légère première catégorisation avait pour objectif de s'assurer que chaque modalité des variables avait une exposition suffisante (> 1000).

D'après la partie 3, les variables explicatives sont toutes les variables, sauf *dens*. Nous allons maintenant nous intéresser aux interactions entre variables.


\vspace*{2cm}


## Interaction entre *brand* et *power*

{r, message = FALSE, warning = FALSE}
donnees_cat %>% group_by(brand, power) %>%
  summarise(nb_sini = sum(claims * expo)) %>%
  ggplot(aes(x = power, y = nb_sini, fill = brand)) + geom_col() +
  labs(x = "power", y = "Nombre de sinistres (corrigé par l'exposition)", fill = "brand")


Ce graphique met en lumière que B1 et B2 sont présents en plus grandes proportions pour une puissance faible. Nous allons créer une nouvelle variable *puissance* prenant deux valeurs possibles : "[0,4]" ou "(4,12]". Puis, nous déterminerons si une interaction entre cette nouvelle variable puissance et la variable *brand* apporterait quelque chose au modèle.

{r, message = FALSE, warning = FALSE}
donnees_interaction <- donnees_cat

# Création d'une variable puissance
donnees_interaction$puissance <-
  as.factor(
    ifelse(
      donnees_interaction$power %in% c("[0,1]", "(1,2]", "(2,3]", "(3,4]"),
      "[0,4]",
      "(4,12]"
    )
  )

ajust_inter_1 <- glm(
  claims ~ interaction(puissance, brand) +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_interaction
)

trace_terme_bivar(
  ajustement = ajust_inter_1,
  nom_terme = "interaction(puissance, brand)",
  nom_variable_2 = "puissance",
  nom_variable_1 = "brand",
  trace_exposition = TRUE,
  nom_exposition = "expo",
  trace_int_conf = TRUE
)


Les résultats obtenus sont peu satisfaisants : les termes d'interaction entre les individus avec une faible puissance ([0,4]) et ceux avec une forte puissance ((4,12]) sont proches, sauf pour les marques B1, B12, B5 et B6.

Nous allons donc créer une nouvelle variable marque qui prend deux valeurs : "B1-B12-B5-B6" ou "Autres marques".
Voyons si une interaction entre cette nouvelle variable et la variable *power* apporterait quelque chose au modèle : 

{r, message = FALSE, warning = FALSE}
# Création d'une nouvelle variable marque
donnees_interaction$marque <-
  as.factor(
    ifelse(
      donnees_interaction$brand %in% c("B1", "B12", "B5", "B6"),
      "B1-B12-B5-B6",
      "Autres marques"
    )
  )

ajust_inter_2 <- glm(
  claims ~ interaction(marque, power) +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_interaction
)
summary(ajust_inter_2)

trace_terme_bivar(
  ajustement = ajust_inter_2,
  nom_terme = "interaction(marque, power)",
  nom_variable_2 = "marque",
  nom_variable_1 = "power",
  trace_exposition = TRUE,
  nom_exposition = "expo",
  trace_int_conf = TRUE
)


Cette fois-ci, les résultats sont plus satisfaisants : on remarque que les coefficients diffèrent entre les deux groupes de la variable *marque*.

Générons donc un GLM en ajoutant cette interaction :

{r ajustement_avec_interaction_1, cache = TRUE}
ajustement_avec_interaction_1 <-
  glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      interaction(marque, power) +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_interaction
  )
summary(ajustement_avec_interaction_1)


Regardons maintenant la variation d'AIC après l'ajout de cette interaction. Pour rappel, sans interaction et sur les m^emes données (*donnees_cat*), l'AIC de *ajustement_1_sans_dens* est égal à `r ajustement_1_sans_dens$aic`.

{r, cache = TRUE}
AIC(ajustement_avec_interaction_1)


L'AIC baisse, ce qui est satisfaisant.


\vspace*{2cm}


## Interaction entre *brand* et *ac*

{r, message = FALSE, warning = FALSE}
donnees_cat %>% group_by(brand, ac) %>%
  summarise(nb_sini = sum(claims * expo)) %>%
  ggplot(aes(x = ac, y = nb_sini, fill = brand)) + geom_col() +
  labs(x = "ac", y = "Nombre de sinistres(corrigé par l'exposition)", fill = "brand")


La marque B12 est beaucoup plus présente pour une valeur de *ac* inférieure à 5. Nous allons donc créer une nouvelle variable *agevehic* qui prend deux valeurs : soit "[0,5]", soit "(5,35]".

Voyons si une interaction entre cette nouvelle variable et la variable *brand* apporterait quelque chose au modèle :

{r, message = FALSE, warning = FALSE}
# Création de la nouvelle variable agevehic
donnees_interaction$agevehic <-
  as.factor(
    ifelse(
      donnees_interaction$ac %in% c("[0,1]", "(1,2]", "(2,3]", "(3,4]", "(4,5]"),
      "[0,5]",
      "(5,35]"
    )
  )

ajust_inter_3 <- glm(
  claims ~ interaction(agevehic, brand) +
    offset(log(expo)),
  family = poisson(link = "log"),
  data = donnees_interaction
)

trace_terme_bivar(
  ajustement = ajust_inter_3,
  nom_terme = "interaction(agevehic, brand)",
  nom_variable_2 = "agevehic",
  nom_variable_1 = "brand",
  trace_exposition = TRUE,
  nom_exposition = "expo",
  trace_int_conf = TRUE
)


Les résultats sont plus satisfaisants : on remarque que les coefficients diffèrent entre les deux groupes de la variable *agevehic*.

Générons donc un GLM en ajoutant cette interaction :

{r ajustement_avec_interaction_2, cache = TRUE}
ajustement_avec_interaction_2 <-
  glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      interaction(agevehic, brand) +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_interaction
  )
summary(ajustement_avec_interaction_2)


Regardons maintenant la variation d'AIC après l'ajout de cette interaction. Pour rappel, sans interaction et sur les m^emes données (*donnees_cat*), l'AIC de *ajustement_1_sans_dens* est égal à `r ajustement_1_sans_dens$aic`.

{r, cache = TRUE}
AIC(ajustement_avec_interaction_2)


L'AIC baisse, ce qui est satisfaisant.


\vspace*{2cm}


## Ajout des deux interactions précédentes

{r ajustement_interaction, cache = TRUE}
ajustement_interaction <-
  glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      interaction(marque, power) +
      interaction(agevehic, brand) +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_interaction
  )

summary(ajustement_interaction)


Regardons maintenant la variation d'AIC après l'ajout des deux interactions. Pour rappel, nous avions un AIC égal à `r ajustement_avec_interaction_1$aic` pour l'interaction entre *brand* et *power*, et de `r ajustement_avec_interaction_2$aic` pour l'interaction entre *brand* et *ac*.

{r, cache = TRUE}
AIC(ajustement_interaction)


Avec l'ajout de ces deux interactions, l'AIC baisse encore, ce qui est satisfaisant.


\vspace*{2cm}


## Conclusion

{r ajustement_lasso_interaction, cache = TRUE}
recat_lasso_interaction = function(donnees){
  
  donnees_cat <- donnees
  
  # On duplique ces 2 variables car on va les fusionner avec Lasso,
  # mais on aura besoin des valeurs de départ pour les termes d'interaction
  donnees_cat$brand_old <-  donnees_cat$brand
  donnees_cat$ac_old <-  donnees_cat$ac
  
  
  
  # Modifications de la partie 2 (catégorisation a priori)
  
  donnees_cat$age <-
    cut(donnees$age,
        breaks = c(18, (20:78), 80, 82, 90),
        include.lowest = TRUE)
  
  donnees_cat$ac <-
    cut(donnees$ac,
        breaks = c((0:20), 35),
        include.lowest = TRUE)
  
  donnees_cat$power <-
    cut(donnees$power,
        breaks = c((0:10), 12),
        include.lowest = TRUE)
  
  donnees_cat$dens <- cut(donnees$dens,
                          breaks = quantile(donnees$dens, seq(0, 1, by = 0.1)),
                          include.lowest = TRUE)
  
  
  
  # Modifications de la partie 4 (catégorisation trouvée par Lasso)
  
  donnees_cat_lasso <- donnees_cat %>% mutate(age = factor(
    case_when(
      age %in% c("(24,25]", "(25,26]") ~ "(24,26]",
      age %in% c("(26,27]", "(27,28]") ~ "(26,28]",
      age %in% c("(28,29]", "(29,30]") ~ "(28,30]",
      age %in% c("(30,31]", "(31,32]", "(32,33]") ~ "(30,33]",
      age %in% c("(33,34]", "(34,35]") ~ "(33,35]",
      age %in% c("(35,36]", "(36,37]", "(37,38]", "(38,39]", "(39,40]") ~ "(35,40]",
      age %in% c("(40,41]", "(41,42]", "(42,43]", "(43,44]", "(44,45]") ~ "(40,45]",
      age %in% c("(45,46]", "(46,47]") ~ "(45,47]",
      age %in% c("(47,48]", "(48,49]") ~ "(47,49]",
      age %in% c(
        "(49,50]",
        "(50,51]",
        "(51,52]",
        "(52,53]",
        "(53,54]",
        "(54,55]",
        "(55,56]"
      ) ~ "(49,56]",
      age %in% c("(56,57]", "(57,58]") ~ "(56,58]",
      age %in% c("(58,59]", "(59,60]", "(60,61]") ~ "(58,61]",
      age %in% c("(61,62]", "(62,63]", "(63,64]", "(64,65]") ~ "(61,65]",
      age %in% c("(66,67]", "(67,68]") ~ "(66,68]",
      age %in% c("(68,69]", "(69,70]", "(70,71]") ~ "(68,71]",
      age %in% c("(71,72]", "(72,73]") ~ "(71,73]",
      age %in% c("(74,75]", "(75,76]", "(76,77]", "(77,78]") ~ "(74,78]",
      TRUE ~ as.character(age)
    )
  )) %>% mutate(ac = factor(
    case_when(
      ac %in% c("(2,3]", "(3,4]", "(4,5]") ~ "(2,5]",
      ac %in% c("(6,7]", "(7,8]", "(8,9]", "(9,10]", "(10,11]", "(11,12]") ~ "(6,12]",
      ac %in% c(
        "(12,13]",
        "(13,14]",
        "(14,15]",
        "(15,16]",
        "(16,17]",
        "(17,18]"
      ) ~ "(12,18]",
      ac %in% c("(18,19]", "(19,20]") ~ "(18,20]",
      TRUE ~ as.character(ac)
    )
  )) %>% mutate(power = factor(
    case_when(
      power %in% c("(1,2]", "(2,3]") ~ "(1,3]",
      power %in% c("(8,9]", "(9,10]", "(10,12]") ~ "(8,12]",
      TRUE ~ as.character(power)
    )
  )) %>% mutate(brand = factor(
    case_when(
      brand %in% c("B1", "B10", "B3", "B6") ~ "B1+B10+B3+B6",
      brand %in% c("B11", "B13", "B5") ~ "B11+B13+B5",
      brand %in% c("B14", "B2", "B4") ~ "B14+B2+B4",
      TRUE ~ as.character(brand)
    )
  )) %>% mutate(ct = factor(
    case_when(
      ct %in% c("AG", "FR", "GL", "GR", "VS", "ZG") ~ "AG+FR+GL+GR+VS+ZG",
      ct %in% c("AI", "JU", "SH", "TG") ~ "AI+JU+SH+TG",
      ct %in% c("BL", "SO", "UR", "ZH") ~ "BL+SO+UR+ZH",
      ct %in% c("GE", "NE", "VD") ~ "GE+NE+VD",
      ct %in% c("LU", "NW", "SG") ~ "LU+NW+SG",
      ct %in% c("OW", "SZ") ~ "OW+SZ",
      TRUE ~ as.character(ct)
    )
  ))
  
  
  
  # Modifications de la partie 5 (ajout de termes d'interaction)
  
  donnees_lasso_interaction <- donnees_cat_lasso %>% mutate(marque =
    factor(
      ifelse(
        brand_old %in% c("B1", "B12", "B5", "B6"),
        "B1-B12-B5-B6",
        "Autres marques"
      )
    )
  ) %>% mutate(agevehic =
    factor(
      ifelse(
        ac_old %in% c("[0,1]", "(1,2]", "(2,3]", "(3,4]", "(4,5]"),
       "[0,5]",
       "(5,35]"
      )
     )
    )
  
  donnees_lasso_interaction
}

donnees_lasso_interaction <- recat_lasso_interaction(donnees)



ajustement_lasso_interaction <-
  glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      interaction(marque, power) +
      interaction(agevehic, brand) +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_lasso_interaction
  )


Le premier ajustement effectué était *ajustement_1* dont l'AIC est de `r ajustement_1$aic`. Le second ajustement, *ajustement_1_sans_dens*, a un AIC de `r ajustement_1_sans_dens$aic`. Le troisième ajustement, *ajustement_lasso*, a un AIC de `r ajustement_lasso$aic`.

*ajustement_interaction* a un AIC de `r ajustement_interaction$aic`. Deux termes d'interaction ont été rajoutés à l'*ajustement_1_sans_dens* : *interaction(marque, power)* et *interaction(agevehic, brand)*. Le jeu de données utilisé est *donnees_interaction* ; c'est *donnees_cat* où deux nouvelles variables (*marque* et *agevehic*) ont été créées.

L'ajustement Lasso auquel les deux termes d'interaction ont été rajoutés (*ajustement_lasso_interaction*) a quant à lui un AIC de `r ajustement_lasso_interaction$aic.



\newpage



# 6/ Approche de type GAM pour l'ajustement d'effets lisses

## Choix manuel du nombre de degrés de liberté

### Pour la variable *age*

Suite à un problème de chargement du code, nous sommes contraints de trouver le nombre de degrés de liberté en considérant seulement une partie de la base de données (10%) :

{r Extrait des données, cache = TRUE}
donnees_extrait <- donnees %>%
  mutate(random = runif(n = n(), 0, 1)) %>%
  filter(random < 0.1) %>% select(-c(random))



{r Splines age, cache = TRUE}
# Ajustement de l'effet de age en utilisant des splines cubiques de degrés de liberté variés

library(splines)
ajustement_spl <- list()
degres <- (1:15)

for (i in (1:length(degres)))
{
  formule <- as.formula(paste(
    "claims~ns(age,",
    i,
    ")+ac+power+gas+brand+area+ct+offset(log(expo))"
  ))
  ajustement_spl[[i]] <-
    glm(formula = formule,
        family = poisson(link = "log"),
        data = donnees_extrait)
}

AIC_spl <-
  sapply(X = ajustement_spl, FUN = AIC) # Calcul des AIC pour chaque ajustement
plot(x = degres,
     y = AIC_spl,
     type = "l",
     col = "red")


Réalisons un zoom sur le graphique :

{r, cache = TRUE}
plot(
  x = degres,
  y = AIC_spl,
  type = "l",
  col = "red",
  ylim = c(38755, 38770)
)
grid()


Nous constatons qu'il vaut mieux conserver un degré de liberté égal à 6 ou 8, qui minimisent l'AIC. Nous allons choisir un degré de liberté égal à 6 pour que le modèle soit le plus simple possible tout en obtenant de bonnes performances.

Calculons également les critères LOOCV approchés pour chaque ajustement :

{r, cache = TRUE}
# Calcul des critères LOOCV approchés pour chaque ajustement
loocv_spl <-
  sapply(X = ajustement_spl, FUN = approx_loocv_deviance_leviers)
plot(x = degres,
     y = loocv_spl,
     type = "l",
     col = "red")


Réalisons encore un fois un zoom sur le graphique :

{r, cache = TRUE}
plot(
  x = degres,
  y = loocv_spl,
  type = "l",
  col = "red",
  ylim = c(28560, 28580)
)
grid()


Le graphique obtenu indique qu'il vaut mieux conserver un degré de liberté égal à 6 ou 8. Nous choisirons donc 6.

Ainsi, le degré de liberté pour la variable *age* sera 6.


\vspace*{2cm}


### Pour la variable *ac*

{r Splines ac, cache = TRUE}
# Ajustement de l'effet de ac en utilisant des splines cubiques de degrés de liberté variés

library(splines)
ajustement_spl <- list()
degres <- (1:15)

for (i in (1:length(degres)))
{
  formule <- as.formula(paste(
    "claims~ns(ac,",
    i,
    ")+age+power+gas+brand+area+ct+offset(log(expo))"
  ))
  ajustement_spl[[i]] <-
    glm(formula = formule,
        family = poisson(link = "log"),
        data = donnees_extrait)
}
AIC_spl <-
  sapply(X = ajustement_spl, FUN = AIC) # Calcul des AIC pour chaque ajustement
plot(x = degres,
     y = AIC_spl,
     type = "l",
     col = "red")


Réalisons un zoom sur le graphique :

{r, cache = TRUE}
plot(
  x = degres,
  y = AIC_spl,
  type = "l",
  col = "red",
  ylim = c(38580, 38600)
)
grid()


Il vaut donc mieux conserver un degré de liberté égal à 11. De plus, concernant les critères LOOCV approchés pour chaque ajustement :

{r, cache = TRUE}
# Calcul des critères LOOCV approchés pour chaque ajustement
loocv_spl <-
  sapply(X = ajustement_spl, FUN = approx_loocv_deviance_leviers)
plot(x = degres,
     y = loocv_spl,
     type = "l",
     col = "red")


Réalisons un zoom sur le graphique :

{r, cache = TRUE}
plot(
  x = degres,
  y = loocv_spl,
  type = "l",
  col = "red",
  ylim = c(28390, 28410)
)
grid()


À nouveau, nous voyons qu'il vaut mieux conserver un degré de liberté égal à 11. Ainsi, le degré de liberté pour la variable *ac* sera 11.



\vspace*{3cm}



## Splines pénalisées et GAM

Une fois les degrés de liberté optimaux trouvés, nous pouvons à présent réaliser un modèle GAM.

{r Ajustement GAM, cache = TRUE, message = FALSE, warning = FALSE}
library(mgcv) # Paquet de référence utilisé pour les GAM
ddl_age <- 6
ddl_ac <- 11

ajustement_gam <-
  gam(
    claims ~ s(age, k = ddl_age) + s(ac, k = ddl_ac) + power + gas + brand + area + ct + offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_extrait,
    control = list(keepData = TRUE)
  )

summary(ajustement_gam)



{r, message = FALSE, warning = FALSE}
# trace_terme_univar(
#   ajustement = ajustement_gam,
#   nom_variable = "age",
#   nom_terme = "s(age)",
#   trace_expo = TRUE,
#   nom_expo = "expo",
#   trace_int_conf = TRUE
# )
# 
# trace_terme_univar(
#   ajustement = ajustement_gam,
#   nom_variable = "ac",
#   nom_terme = "s(ac)",
#   trace_expo = TRUE,
#   nom_expo = "expo",
#   trace_int_conf = TRUE
# )

M^eme si cet ajustement GAM donne de bons résultats pour les âges du conducteur inférieurs à 80 ans et pour les âges du véhicule inférieurs à 15 ans, on peut noter une grande incertitude des termes lisses au-delà de ces âges.

**Du fait de cette forte imprécision, nous ne considérerons pas comme modèle final un modèle comportant des effets lisses.**



\newpage


# 7/ Qualité d'ajustement des modèles étudiés

Note : Beaucoup de graphiques de cette partie étaient beaucoup trop volumineux, le fichier PDF total était de 800 Mo. Nous les avons mis en commentaire pour avoir un fichier PDF moins lourd.

## Pour l'ajustement avec Lasso

### Examen des résidus

#### Résidus de Pearson

##### Tracé de base

Afin de vérifier l'adéquation entre modèle et données, on cherche à vérifier que les résidus de Pearson se comportent, au moins approximativement, comme des variables aléatoires indépendantes centrées et de variance unité (ici, $\phi=1$ car on travaille avec la loi de Poisson).

On commence donc par calculer les résidus de Pearson.

{r, cache = TRUE}
# Les résidus de Pearson
res_P_lasso <-
  residuals(ajustement_lasso, type = "pearson")

mean(res_P_lasso)
sd(res_P_lasso)


La valeur de la moyenne empirique n'est pas strictement égale à 0 (alors que la valeur totale observée est exactement égale à l'espérance totale modélisée), du fait des normalisations effectuées dans la définition des résidus de Pearson. L'écart-type empirique est quant à lui voisin de 1.

On calcule également, pour chaque ligne du jeu de données, la valeur de l'espérance modélisée de la réponse normalisée par l'exposition.

{r, cache = TRUE}
# Valeur moyenne prédite normalisée par l'exposition
val_pred_n_lasso <-
  fitted(ajustement_fus_lasso) / donnees_cat_lasso$expo


On peut alors procéder au tracé mettant en abscisse la valeur moyenne prédite et en ordonnée le résidu de Pearson :

{r TROP GROS 1, cache = TRUE}
# plot(
#   x = val_pred_n_lasso,
#   y = res_P_lasso,
#   main = "Résidus de Pearson",
#   ylab = "Résidu",
#   xlab = "Espérance modélisée normalisée",
#   col = adjustcolor("black", alpha = 0.5),
#   cex = 0.2
# )
# abline(h = 0, col = "red")


On cherche alors à vérifier que les résidus de Pearson sont approximativement centrés et de variance 1 le long du tracé, mais une vérification visuelle directe de ces propriétés à partir du graphique ci-dessus n'a rien d'évident.

On peut néanmoins déjà remarquer que les points se regroupent sur des hyperboles, et que des amas de points sont visibles (l'un en-dessous de l'axe des abscisses, et un autre au-dessus).

{r, cache = TRUE}
couleurs <-
  rainbow(10)[donnees_cat_lasso$claims + 1] # Les couleurs pour le tracé
# plot(
#   x = val_pred_n_lasso,
#   y = res_P_lasso,
#   main = "Résidus de Pearson",
#   ylab = "résidu",
#   xlab = "espérance modélisée normalisée",
#   col = adjustcolor(couleurs, alpha = 0.5),
#   cex = 0.2
# )
# abline(h = 0, col = "red")


{r, cache = TRUE}
lval_pred_n_lasso <-
  log(val_pred_n_lasso) # Echelle log pour les valeurs prédites
# plot(
#   x = lval_pred_n_lasso,
#   y = res_P_lasso,
#   cex = 0.2,
#   main = "Résidus de Pearson",
#   ylab = "résidu",
#   xlab = "log(espérance modélisée normalisée)",
#   col = adjustcolor("black", alpha = 0.5)
# )
# abline(h = 0, col = "red")


{r, cache = TRUE}
# plot(
#   x = lval_pred_n_lasso,
#   y = res_P_lasso,
#   cex = 0.2,
#   main = "Résidus de Pearson",
#   ylab = "résidu",
#   xlab = "log(espérance modélisée normalisée)",
#   col = adjustcolor(couleurs, alpha = 0.5)
# )
# abline(h = 0, col = "red")




#####  Utilisation de moyennes lissées

{r BEAUCOUP TROP GROS, cache = TRUE}
#install.packages("locfit")
library("locfit")
attributes(res_P_lasso)[["nom"]] <- "résidu de Pearson"
attributes(lval_pred_n_lasso)[["nom"]] <-
  "log(espérance modélisée normalisée)"
# trace_stat_residus_quantit(
#   residus = res_P_lasso,
#   variable = lval_pred_n_lasso,
#   ylim = c(-3, 25),
#   cex = 0.2
# )
# 
# trace_stat_residus_quantit(
#   residus = res_P_lasso,
#   variable = lval_pred_n_lasso,
#   ylim = c(-2, 2),
#   cex = 0.2
# )
# 
# trace_stat_residus_quantit(
#   residus = res_P_lasso,
#   variable = lval_pred_n_lasso,
#   ylim = c(-0.4, 0.4),
#   cex = 0.2
# )


Les tracés obtenus ne sont pas exactement en accord avec les propriétés attendues lorsque le modèle est valide.

En effet, on peut remarquer que la valeur de référence 0 n'appartient pas partout à l'intervalle de confiance à 95% autour de la moyenne lissée. De m^eme, la valeur de référence ±1 n'appartient pas non plus partout à l'intervalle de confiance à 95% autour de la racine carrée de la moyenne lissée des carrés.

On peut également remarquer que l'incertitude (reflétée par la taille des bandes de confiance) est importante aux extrémités, en particulier pour les bandes de confiance autour de la racine carrée de la moyenne lissée des carrés.

#### Résidus de Pearson agrégés

On choisit ici d'avoir recours à 500 “paquets” sur lesquels seront calculés les résidus agrégés, il s'agit d'un compromis permettant d'avoir à la fois une exposition substantielle par paquet, et un nombre raisonnable de résidus à examiner.

{r, cache = TRUE}
nb_classes <- 500
comparaison <- obs_pred_glm(
  ajustement = ajustement_fus_lasso,
  nb_classes = nb_classes,
  nom_reponse = "claims",
  nom_expo = "expo",
  calc_variance = TRUE
)


On vérifie rapidement l'homogénéité de l'exposition des classes ainsi définies.

{r, cache = TRUE}
hist(comparaison[["exposition"]])


On calcule maintenant les résidus de Pearson agrégés en appliquant, pour chaque classe, la formule définissant ceux-ci.

{r, cache = TRUE}
# Valeur moyenne prédite (normalisée par l'exposition)
val_pred_n_lasso <-
  comparaison[["val_pred"]] / comparaison[["exposition"]]
# Valeur observée (normalisée par l'exposition
val_obs_n_lasso <-
  comparaison[["reponse"]] / comparaison[["exposition"]]
# Résidus de Pearson agrégés
res_P_crunch_lasso <-
  (comparaison[["reponse"]] - comparaison[["val_pred"]]) / sqrt(comparaison[["variance_pred"]])

plot(
  x = val_pred_n_lasso,
  y = res_P_crunch_lasso,
  main = "Résidus de Pearson agrégés",
  xlab = "espérance modélisée (normalisée)",
  ylab = "résidu"
)


On passe ensuite en échelle logarithmique sur les abscisses, pour mieux étaler les points.

{r, cache = TRUE}
plot(
  x = val_pred_n_lasso,
  y = res_P_crunch_lasso,
  main = "Résidus de Pearson agrégés",
  xlab = "espérance modélisée (normalisée)",
  ylab = "résidu",
  log = "x",
  ylim = c(-4, 4)
)


On calcule maintenant deux seuils de valeurs calculés par approximation normale : un seuil ayant une probabilité 95% de ne pas ^etre dépassé, individuellement pour chaque résidu, et un seuil global ayant une probabilité d'environ 95% de n'^etre dépassé par aucun des résidus.

{r, cache = TRUE}
nb <- nb_classes # Nombre de résidus
alpha <- 0.05 # Niveau de risque souhaité
# Seuil individuel
seuil <- qnorm(p = (1 - alpha / 2))
# Seuil avec borne sur la réunion
seuil_union <- qnorm(p = (1 - alpha / (2 * nb)))
#Tracé des seuils
plot(
  x = val_pred_n_lasso,
  y = res_P_crunch_lasso,
  main = "Résidus de Pearson agrégés",
  xlab = "espérance modélisée (normalisée)",
  ylab = "résidu",
  log = "x",
  ylim = c(-4, 4)
)
abline(h = 0, lty = 3)
abline(h = c(-seuil, seuil),
       lty = 3,
       col = "blue")
abline(h = c(-seuil_union, seuil_union),
       lty = 3,
       col = "red")

On ne relève visuellement de manière évidente ni points aberrants, ni tendance sur l'espérance ou la variance, ni sur- ou sous-dispersion, globale, ni dissymétrie.

{r, cache = TRUE}
hist(res_P_crunch_lasso, prob = TRUE)
grille <- seq(-4, 4, 0.02)
lines(grille, dnorm(x = grille), col = "red")


Une superposition de l'histogramme des résidus et de la densité de la loi N(0,1) ne révèle pas non plus de problème flagrant.

{r, cache = TRUE}
plot(ecdf(res_P_crunch_lasso))
grille <- seq(-4, 4, 0.02)
lines(grille, pnorm(q = grille), col = "red")


De m^eme avec une comparaison entre fonction de répartition empirique des résidus et fonction de répartition de la loi N(0,1).



On conclut avec un tracé quantile-quantile normal agrémenté d'une enveloppe de prédiction à 95% (calculée par approximation gaussienne), qui ne suggère pas non plus de problème majeur.


{r, cache = TRUE}
library(car)
qqPlot(res_P_crunch_lasso, id = list(n = 0), envelope = 0.95)


#### Résidus quantile normalisés randomisés

##### Calcul et tracé de base

On considère maintenant les résidus quantile normalisés (et randomisés, puisque notre modèle est basé sur une loi discrète).

{r, cache = TRUE}
#install.packages("statmod")
library(statmod)
res_Q_lasso <- qresiduals(ajustement_fus_lasso)



On calcule les valeurs moyennes modélisées normalisées par l'exposition, et leur logarithme.


{r, cache = TRUE}
# Valeur moyenne prédite normalisée par l'exposition
val_pred_n_lasso <-
  fitted(ajustement_fus_lasso) / donnees[["expo"]]
# Echelle log pour les valeurs prédites
lval_pred_n_lasso <- log(val_pred_n_lasso)


On effectue maintenant le tracé des résidus en fonction de la valeur moyenne prédite.

{r, cache = TRUE}
# plot(
#   x = val_pred_n_lasso,
#   y = res_Q_lasso,
#   main = "Résidus quantile randomisés normalisés",
#   xlab = "espérance modélisée (normalisée)",
#   ylab = "résidu",
#   col = adjustcolor("black", alpha = 0.5),
#   cex = 0.05
# )
# grid(col = "purple")

On ne remarque plus d'amas (comme observé sur les résidus de Pearson), et la répartition des points est visiblement beaucoup plus symétrique.

On passe à une échelle logarithmique en abscisse, afin d'étaler davantage les points.

{r, cache = TRUE}
# plot(
#   x = val_pred_n_lasso,
#   y = res_Q_lasso,
#   main = "Résidus quantiles randomisés normalisés",
#   xlab = "espérance modélisée (normalisée)",
#   ylab = "résidu",
#   col = adjustcolor("black", alpha = 0.5),
#   cex = 0.05,
#   log = "x"
# )
# grid(col = "purple")


On procède maintenant à un tracé des moyennes lissées, du m^eme type que celui pratiqué avec les résidus de Pearson.

{r, cache = TRUE}
# attributes(res_Q_lasso) <-
#   list(nom = "résidu quantile randomisé normalisé")
# attributes(lval_pred_n_lasso)[["nom"]] <-
#   "log(espérance modélisée normalisée)"
# trace_stat_residus_quantit(
#   residus = res_Q_lasso,
#   variable = lval_pred_n_lasso,
#   ylim = c(-2, 2),
#   cex = 0.05
# )
# 
# trace_stat_residus_quantit(
#   residus = res_Q_lasso,
#   variable = lval_pred_n_lasso,
#   ylim = c(-0.5, 0.5),
#   cex = 0.05
# )


Les conclusions sont cette fois-ci meilleurs que celles obtenues sur les résidus de Pearson. On remarque en effet que la valeur de référence 0 appartient en grande partie à l'intervalle de confiance à 95% autour de la moyenne lissée. On peut également constater que les intervalles de confiances sont plus fins.

##### Utilisation d'une échelle de couleurs pour la densité locale

On présente dans cette partie l'utilisation d'une échelle de couleurs pour représenter la densité locale de points observée sur le graphique, et étudier ainsi la normalité des résidus considérés.

On commence par définir une palette de couleurs qui servira pour représenter la densité, de bleu (faible valeur) à rouge (forte valeur).

{r, cache = TRUE}
jolie_palette <-
  colorRampPalette(
    c(
      "#00007F",
      "blue",
      "#007FFF",
      "cyan",
      "#7FFF7F",
      "yellow",
      "#FF7F00",
      "red",
      "#7F0000"
    )
  )

couleurs <-
  densCols(
    x = lval_pred_n_lasso,
    y = res_Q_lasso,
    nbin = 512,
    colramp = jolie_palette
  )


On reprend maintenant le tracé de la partie précédente, en colorant chaque point à l'aide de la couleur ainsi calculée, ce qui fournit une visualisation en couleurs de la densité locale de points.

{r, cache = TRUE}
# plot(
#   x = val_pred_n_lasso,
#   y = res_Q_lasso,
#   main = "Résidus quantiles randomisés normalisés",
#   xlab = "espérance modélisée (normalisée)",
#   ylab = "résidu",
#   col = couleurs,
#   cex = 0.2,
#   log = "x"
# )


A ce stade, on visualise bien la symétrie de la distribution des résidus le long de l'axe horizontal. On peut aussi remarquer la normalité, m^eme si l'espérance modélisée s'étalent un peu trop à droite.

Une approche possible est de caler les coordonnées horizontales sur les valeurs triées d'un échantillon normal de m^eme taille, puis de comparer les lignes de niveau de la densité bi-variée à des cercles, qui correspondent au cas d'une loi normale bivariée centrée de matrice de variance-covariance identité.

Selon ce principe, on calcule ci-dessous de nouvelles coordonnées horizontales x_rand_norm.

{r, cache = TRUE}
# Cette fonction randomise-normalise les abscisses (en conservant l'ordre)
fonc_rand_norm <- function(x)
{
  rangs_x <- rank(x, ties.method = "random")
  x_rand <- sort(rnorm(n = length(x))[rangs_x])
  return(x_rand)
}
x_rand_norm_lasso <- fonc_rand_norm(lval_pred_n_lasso)


On effectue maintenant le tracé de la densité locale de points en utilisant ces nouvelles coordonnées horizontales. Les lignes de niveau dessinent les cercles concentriques attendus, fournissant une vérification graphique de la normalité des résidus.

{r TROP GROS}
# Tracés avec les abscisses randomisées-normalisées
couleurs <- densCols(
  x = x_rand_norm_lasso,
  y = res_Q_lasso,
  nbin = 512,
  colramp = jolie_palette
)
# plot(
#   x = x_rand_norm_lasso,
#   y = res_Q_lasso,
#   main = "Résidus quantiles randomisés normalisés",
#   xlab = "espérance modélisée rand. norm.",
#   ylab = "résidu",
#   col = couleurs,
#   cex = 0.2,
#   asp = 1
# )


On a ainsi obtenu une vérification non seulement du fait que la distribution globale des résidus suit (approximativement) une loi normale, mais encore que cette propriété de normalité est vérifiée tout au long de l'axe horizontal du tracé.




### Distance de Cook

On calcule maintenant les distances de Cook.


{r, cache = TRUE}
# Récupération des distances de Cook
dist_Cook_lasso <- cooks.distance(ajustement_fus_lasso) 


Visualisons la distribution des distances de Cook.

{r, cache = TRUE}
hist(dist_Cook_lasso, breaks = 20)


Voici le tracé de la distance de Cook en fonction de l'indice :

{r, cache = TRUE}
plot(dist_Cook_lasso, type = "h", ylab = "distance de Cook")


On reprend le m^eme tracé en triant les distances de Cook par ordre décroissant.

{r, cache = TRUE}
plot(sort(dist_Cook_lasso, decreasing = TRUE),
     type = "h",
     ylab = "distances de Cook ordonnées") 


On peut maintenant extraire les 10 lignes associées aux plus grandes valeurs de la distance de Cook.

{r, cache = TRUE}
indices_max_lasso <- order(dist_Cook_lasso, decreasing = TRUE)[1:10]
indices_max_lasso


On peut remarquer sur le dernier tracé que peu de valeurs ont une distance de Cook supérieure à 0.0005. Nous allons donc les récupérer, car ce sont ces données qui ont un grand impact individuel sur l'estimation des coefficients du modèle.

{r, cache = TRUE}
dist_Cook_lasso[dist_Cook_lasso > 0.0005]




\newpage




# 8/ Performances prédictives des modèles étudiés 

Suite aux différentes parties précédentes, nous arrivons à la recatégorisation suivante des variables :

{r Données finales, cache = TRUE}
donnees_lasso_interaction <- recat_lasso_interaction(donnees)
# Cf. la fin de la partie 5 pour la définition de cette fonction

donnees_finales <- donnees_lasso_interaction
head(donnees_finales)


L'ajustement final est le suivant :

{r Ajustement final, cache = TRUE}
ajustement_final <-
  glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      interaction(marque, power) +
      interaction(agevehic, brand) +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_finales
  )

summary(ajustement_final)



\vspace*{2cm}


## Utilisation du jeu de données de test

{r, cache = TRUE}
donnees_test = read.csv(url("http://www-irma.u-strasbg.fr/~jberard/MTPL_donnees_test.csv"))
summary(donnees_test)
str(donnees_test)


Il est d'abord nécessaire de recréer nos catégories. En effet, il faut faire correspondre (pour l'âge par exemple) "(18,20]" aux valeurs "18", "19" et "20" de la variable.

{r, cache = TRUE}
donnees_test <- recat_lasso_interaction(donnees_test)
head(donnees_test)


{r, cache = TRUE}
# Extraction de la fonction de déviance
fonc_deviance <- ajustement_final$family$dev.resids



calcul_deviance_eq <- function(donnees) {
  deviance <- list(tot = NULL, moy = NULL)
  erreur_quadratique <- list(tot = NULL, moy = NULL)
  
  
  # Partie 0 : Calcul des termes
  
  # Calcul des espérances modélisées sur les données
  mu_pred <-
    predict(ajustement_final, newdata = donnees, type = "response")
  
  # Extraction des réponses observées sur les données
  reponse <- donnees$claims
  
  # Normalisation par l'exposition des valeurs précédentes
  expo <- donnees$expo
  mu_pred_n <- mu_pred / expo
  reponse_n <- reponse / expo
  
  
  # Partie 1 : Déviance
  
  # Calcul des déviances sur chaque ligne du jeu de données
  dev <- fonc_deviance(reponse_n, mu_pred_n, wt = expo)
  # Déviance totale sur les données
  deviance$tot <- sum(dev)
  # Déviance moyenne sur les données
  deviance$moy <- mean(dev)
  
  
  # Partie 2 : Erreur quadratique
  
  # Erreur quadratique sur chaque ligne du jeu de données
  eq <- expo * (reponse_n -  mu_pred_n) ^ 2
  # Erreur quadratique totale sur les données
  erreur_quadratique$tot <- sum(eq)
  # Erreur quadratique moyenne sur les données
  erreur_quadratique$moy <- mean(eq)
  
  
  resultat <- list(dev = deviance, eq = erreur_quadratique)
  resultat
}

resultat_test <- calcul_deviance_eq(donnees_test)
resultat_ajust <- calcul_deviance_eq(donnees_finales)


\vspace*{1cm}

### Déviance

Commençons par la comparaison de la déviance.

{r, cache = TRUE}
resultat_test$dev
resultat_ajust$dev


\vspace*{1cm}

### Erreur quadratique

Passons maintenant à la comparaison de l'erreur quadratique :

{r, cache = TRUE}
resultat_test$eq
resultat_ajust$eq


\vspace*{1cm}

### Conclusion

Les valeurs moyennes obtenues sur les données de test et sur les données d'ajustement sont proches, ceci tant pour la déviance que l'erreur quadratique.

Ces résultats suggèrent qu'il n'y a pas de sur-ajustement du modèle. De plus, comme attendu, les valeurs obtenues sur les données d'ajustement sont à chaque fois (très légèrement) inférieures à celles obtenues sur les données de test. Ce résultat était attendu, car le modèle a été appris sur les données d'ajustement, ce sont donc avec ces données qu'il performe le mieux.

Plus précisément, les écarts relatifs entre les valeurs obtenues sont les suivants :

{r, cache = TRUE}

(resultat_test$dev$moy - resultat_ajust$dev$moy)/resultat_ajust$dev$moy

(resultat_test$eq$moy - resultat_ajust$eq$moy)/resultat_ajust$eq$moy



\vspace*{2cm}


## Comparaison avec les résultats obtenus pour un autre modèle

On calcule les m^emes mesures de performance prédictive pour le modèle GLM sans interaction.

{r, cache = TRUE}
# Comparaison avecle modèle de base (sans regroupement de variable ni interaction)
ajustement_sans_interaction <-glm(
    claims ~ age + ac + power + gas +
      brand + area + ct +
      offset(log(expo)),
    family = poisson(link = "log"),
    data = donnees_finales
  )


{r, cache = TRUE}
mu_pred_test_sans_interaction <-
  predict(ajustement_sans_interaction,
          newdata = donnees_test,
          type = "response")
reponse_test_sans_interaction <- donnees_test$claims
expo_test <- donnees_test$expo

mu_pred_test_n_sans_interaction <-
  mu_pred_test_sans_interaction / expo_test
reponse_test_n_sans_interaction <-
  reponse_test_sans_interaction / expo_test
dev_test_sans_interaction <-
  fonc_deviance(reponse_test_n_sans_interaction,
                mu_pred_test_n_sans_interaction,
                wt = expo_test)
dev_test_tot_sans_interaction <- sum(dev_test_sans_interaction)
dev_test_tot_sans_interaction


{r, cache = TRUE}
# Déviance moyenne sur les données de test
dev_test_moy_sans_interaction <- mean(dev_test_sans_interaction)
dev_test_moy_sans_interaction


Pour rappel, les valeurs obtenues pour le modèle avec les interactions étaient les suivantes :

{r, cache = TRUE}
resultat_test$dev
resultat_ajust$dev


On constate ainsi la dégradation des performances prédictives lorsque les interactions entre variables sont négligées, par le fait que les déviance totale et moyenne augmentent.

On reprend le calcul avec l'erreur quadratique.

{r, cache = TRUE}
eq_test_sans_interaction <-
  expo_test * (reponse_test_n_sans_interaction -  mu_pred_test_n_sans_interaction) ^
  2
# Erreur quadratique totale sur les données de test
eq_test_tot_sans_interaction <- sum(eq_test_sans_interaction)
eq_test_tot_sans_interaction


{r, cache = TRUE}
# Erreur quadratique moyenne sur les données de test
eq_test_moy_sans_interaction <- mean(eq_test_sans_interaction)
eq_test_moy_sans_interaction


Les valeurs obtenues pour le modèle incluant les interactions étaient les suivantes :

{r, cache = TRUE}
resultat_test$eq
resultat_ajust$eq


On remarque encore une fois la dégradation des performances prédictives lorsque les interactions entre variables sont négligées, par le fait que les erreurs quadratiques totale et moyenne augmentent.


\vspace*{2cm}


## Autres critères : validation croisée et AIC

Calculer de manière explicite le critère de validation croisée “leave-one-out” représenterait ici un temps de calcul prohibitif, car cela supposerait de réajuster le modèle autant de fois qu'il y a de lignes.

On préfèrera donc utiliser l'approximation basée sur les leviers.

{r, cache = TRUE}
# Validation croisée pour la déviance en utilisant l'approximation par les leviers
resultat_approx_cv <- approx_loocv_deviance_leviers(ajustement_final)
resultat_approx_cv


On calcule maintenant un critère de validation croisée “K-fold” avec K=10.

{r, message = FALSE, warning = FALSE, cache = TRUE}
# Validation croisée K-fold avec K=10
resultat_10_fold_cv <-
  cv.glm_modif(data = donnees_finales, glmfit = ajustement_final, K = 10)
resultat_10_fold_cv


On calcule maintenant le critère de validation croisée généralisée.

{r, cache = TRUE}
# Validation croisée généralisée GCV
resultat_gcv <- glm_deviance_GCV(ajustement_final)
resultat_gcv


Pour terminer avec le critère AIC, d'abord extrait par une fonction dédiée, puis recalcul manuel :

{r, cache = TRUE}
# AIC
resultat_AIC<-AIC(ajustement_final)
resultat_AIC
# Calcul manuel
-2*as.numeric(logLik(ajustement_final))+2*ajustement_final$rank


On calcule de nouveau les quantités précédentes pour le modèle sans interaction.

{r, cache = TRUE}
resultat_approx_cv_sans_interaction <-
  approx_loocv_deviance_leviers(ajustement_sans_interaction)
resultat_approx_cv_sans_interaction


{r, cache = TRUE}
resultat_10_fold_cv_sans_interaction <-
  cv.glm_modif(data = donnees_finales, glmfit = ajustement_sans_interaction, K =
                 10)
resultat_10_fold_cv_sans_interaction



{r, cache = TRUE}
resultat_gcv_sans_interaction <-
  glm_deviance_GCV(ajustement_sans_interaction)
resultat_gcv_sans_interaction


{r, cache = TRUE}
resultat_AIC_sans_interaction <- AIC(ajustement_sans_interaction)
resultat_AIC_sans_interaction


Les valeurs obtenues sont supérieures à celles obtenues pour le modèle avec interaction. Cela montre encore une fois la dégradation des performances prédictives lorsque les interactions entre variables sont négligées.


\vspace*{2cm}


## Courbes de Lorenz

On procède maintenant au calcul et au tracé de la courbe de Lorenz associée, calculée sur les données de test.

{r, cache = TRUE}
courbe_Lorenz <- Lorenz_curve_test_glm(
  ajustement = ajustement_final,
  donnees_test = donnees_test,
  nom_reponse = "claims",
  nom_exposition = "expo"
)
plot(
  courbe_Lorenz$cumul_exposition,
  courbe_Lorenz$cumul_reponse,
  main = "Courbe de Lorenz sur données test",
  xlab = "Exposition cumulée",
  ylab = "Réponse cumulée",
  type = "l",
  asp = 1,
  xlim = c(0, 1),
  ylim = c(0, 1)
)
lines(
  x = c(0, tail(courbe_Lorenz$cumul_exposition, 1)),
  y = c(0, tail(courbe_Lorenz$cumul_reponse, 1)),
  col = "red",
  lty = 2
)
grid()


On trace également la courbe obtenue avec un modèle sans interaction entre variables.

{r, cache = TRUE}
# Comparaison avec un modèle sans interaction
courbe_Lorenz_2 <-
  Lorenz_curve_test_glm(
    ajustement = ajustement_sans_interaction,
    donnees_test = donnees_test,
    nom_reponse = "claims",
    nom_exposition = "expo"
  )
plot(
  courbe_Lorenz$cumul_exposition,
  courbe_Lorenz$cumul_reponse,
  main = "Courbe de Lorenz sur données test",
  xlab = "Exposition cumulée",
  ylab = "Réponse cumulée",
  type = "l",
  asp = 1,
  xlim = c(0, 1),
  ylim = c(0, 1)
)
lines(
  x = c(0, tail(courbe_Lorenz$cumul_exposition, 1)),
  y = c(0, tail(courbe_Lorenz$cumul_reponse, 1)),
  col = "red",
  lty = 2
)
grid()
lines(
  x = courbe_Lorenz_2$cumul_exposition,
  y = courbe_Lorenz_2$cumul_reponse,
  col = "blue"
)


On ajoute également un tracé de courbe de Lorenz “extr^eme”, à titre de comparaison.

{r, cache = TRUE}
# Courbe de Lorenz extr^eme (réponse moyenne prédite = réponse observée)
courbe_Lorenz_extr <- Lorenz_curve_extr(
  donnees_test = donnees_test,
  nom_reponse = "claims",
  nom_exposition = "expo"
)

plot(
  courbe_Lorenz$cumul_exposition,
  courbe_Lorenz$cumul_reponse,
  main = "Courbe de Lorenz sur données test",
  xlab = "Exposition cumulée",
  ylab = "Réponse cumulée",
  type = "l",
  asp = 1,
  xlim = c(0, 1),
  ylim = c(0, 1)
)
lines(
  x = c(0, tail(courbe_Lorenz$cumul_exposition, 1)),
  y = c(0, tail(courbe_Lorenz$cumul_reponse, 1)),
  col = "red",
  lty = 2
)
grid()
lines(
  x = courbe_Lorenz_2$cumul_exposition,
  y = courbe_Lorenz_2$cumul_reponse,
  col = "blue"
)
lines(
  x = courbe_Lorenz_extr$cumul_exposition,
  y = courbe_Lorenz_extr$cumul_reponse,
  col = "green"
)
legend(
  x = "topleft",
  legend = c(
    "ajustement",
    "ajustement sans interaction",
    "cas extr\u{00EA}me",
    "proportionnel expo."
  ),
  col = c("black", "blue", "green", "red"),
  lty = c(1, 1, 1, 2)
)


On effectue maintenant le calcul des aires délimitées par les différentes courbes, ainsi que les rapports entre celles-ci.

{r, cache = TRUE}
# Calculs d'aires
aire_sous_courbe <- aire_sous_courbe_Lorenz(courbe_Lorenz)
aire_sous_courbe

aire_sous_courbe_2 <- aire_sous_courbe_Lorenz(courbe_Lorenz_2)
aire_sous_courbe_2

aire_sous_courbe_extr <- aire_sous_courbe_Lorenz(courbe_Lorenz_extr)
aire_sous_courbe_extr

ratio_aire <- (0.5 - aire_sous_courbe) / (0.5 - aire_sous_courbe_extr)
ratio_aire

ratio_aire_2 <- (0.5 - aire_sous_courbe_2) / (0.5 - aire_sous_courbe_extr)
ratio_aire_2


On remarque pour la courbe sans interaction est légèrement au-dessus de la courbe avec interaction. On conclut donc encore une fois que négliger les interactions entre variables dégrade des performances prédictives du modèle.



\newpage



# 9/ Conclusion

En définitive, la modélisation retenue est celle obtenue grâce à Lasso combinée aux termes d'interaction.

{r}
summary(ajustement_lasso_interaction)


#NoEnv  ; Recommended for performance and compatibility with future AutoHotkey releases.
; #Warn  ; Enable warnings to assist with detecting common errors.
SendMode Input  ; Recommended for new scripts due to its superior speed and reliability.
SetWorkingDir %A_ScriptDir%  ; Ensures a consistent starting directory.
